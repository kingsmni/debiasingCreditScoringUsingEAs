{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from platypus import Hypervolume, display, calculate, CMAES, MOEAD, NSGAII, NSGAIII, SPEA2, IBEA, Problem, Real\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "from bias_functions import *\n",
    "from utility_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(setSelection):\n",
    "    if setSelection == 'toy':\n",
    "        X,y = generate_toy_data(1000,200,2)\n",
    "    elif setSelection == 'adult':\n",
    "        protectedAttributes={'race':'White','gender':'Male'}\n",
    "        X,y = load_adult(protectedAttributes=protectedAttributes)\n",
    "    elif setSelection == 'bank':\n",
    "        X,y = load_bank(smaller=False)\n",
    "    elif setSelection == 'german':\n",
    "        X,y = load_german()\n",
    "    elif setSelection == 'mortgage':\n",
    "        protectedCategoricalAttributes={'applicant_ethnicity_name':'Not Hispanic or Latino',\n",
    "                                'applicant_race_name_1':'White','applicant_sex_name':'Male'}\n",
    "        protectedNumericalAttributes=['minority_population']\n",
    "        X,y = load_mortgage(protectedCategoricalAttributes=protectedCategoricalAttributes, \\\n",
    "                            protectedNumericalAttributes=protectedNumericalAttributes)\n",
    "    else:\n",
    "        print('dataset not recognised')\n",
    "        \n",
    "    X = np.hstack([X, np.ones((X.shape[0],1))]) ## add ones to solve for affine functions\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new problems\n",
    "\n",
    "# problem 4: all datasets and {age,gender,race} [change dataset in main loop ; change senstitive attribute in _train and _test functions]\n",
    "# objectives: accuracy, DI, EO, DM(OMR)\n",
    "\n",
    "def problem4_base(w,X,y,sensitiveAttributeIndex):\n",
    "    errorRateObjective = errorRate(w,X,y)\n",
    "    fairnessObjective1 = differenceDisparateImpactModel(w,X,sensitiveAttributeIndex=sensitiveAttributeIndex)\n",
    "    fairnessObjective2 = differenceEqualOpportunity(w,X,y,sensitiveAttributeIndex=sensitiveAttributeIndex)\n",
    "    fairnessObjective3 = differenceDisparateMistreatment(w,X,y,sensitiveAttributeIndex=sensitiveAttributeIndex,type='OMR')\n",
    "    return errorRateObjective, np.abs(fairnessObjective1[0]-fairnessObjective1[1]),\\\n",
    "np.abs(fairnessObjective2[0]-fairnessObjective2[1]), np.abs(fairnessObjective3[0]-fairnessObjective3[1])\n",
    "\n",
    "def problem4_train(w):\n",
    "    return problem4_base(w,trainxs,trainys,sensitiveAttributeIndex=0)\n",
    "def problem4_test(w):\n",
    "    return problem4_base(w,testxs,testys,sensitiveAttributeIndex=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetValue = 0.01\n",
    "targetMeasureArray = np.array([[np.inf,np.inf,np.inf],[targetValue,np.inf,np.inf],\\\n",
    "                             [np.inf,targetValue,np.inf],[np.inf,np.inf,targetValue],\\\n",
    "                             [targetValue,targetValue,np.inf],[targetValue,np.inf,targetValue],\\\n",
    "                             [np.inf,targetValue,targetValue],[targetValue,targetValue,targetValue]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitive attributes list:\n",
    "\n",
    "toy: 2 ; variables = 4\n",
    "\n",
    "adult: 8 - race, 9 - gender ;  variables = 15 ; 45222 samples\n",
    "\n",
    "bank: 0 - age ; variables = 21 ; 41188 samples\n",
    "\n",
    "german: 6 - gender, 9 - age, 14 - foreign worker ; variables = 25 ; 1000 samples\n",
    "\n",
    "mortgage: 12 - ethnicity, 14 - race, 16 - gender, 24 - minority population ; variables = 30 ; 200000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41188, 21)\n",
      "(41188, 20)\n",
      "age: 0\n",
      "A smaller version of the dataset is loaded...\n",
      "0: training took 219.58 seconds w/ hypervolume 0.978955\n",
      "0: testing took 105.34 seconds w/ hypervolume 0.965577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nigelkingsman/opt/miniconda3/lib/python3.7/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: training took 511.51 seconds w/ hypervolume 0.980903\n",
      "1: testing took 117.84 seconds w/ hypervolume 0.855352\n",
      "2: training took 60.10 seconds w/ hypervolume 0.978075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nigelkingsman/Documents/Education/UCL ML/Project/algorithm auditing/myCode/fairness_functions.py:125: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  p_yHatEqualY_z1 = n_yHatEqualY_z1 / n_z1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: testing took 0.42 seconds w/ hypervolume 0.000000\n",
      "3: training took 732.22 seconds w/ hypervolume 0.977082\n",
      "3: testing took 761.90 seconds w/ hypervolume 0.962118\n",
      "4: training took 112.51 seconds w/ hypervolume 0.978329\n",
      "4: testing took 316.98 seconds w/ hypervolume 0.963974\n",
      "5: training took 92.09 seconds w/ hypervolume 0.979081\n",
      "5: testing took 55.02 seconds w/ hypervolume 0.913143\n",
      "6: training took 77.53 seconds w/ hypervolume 0.979751\n",
      "6: testing took 0.39 seconds w/ hypervolume 0.000000\n",
      "7: training took 66.13 seconds w/ hypervolume 0.977881\n",
      "7: testing took 0.42 seconds w/ hypervolume 0.000000\n",
      "8: training took 1611.98 seconds w/ hypervolume 0.978296\n",
      "8: testing took 441.72 seconds w/ hypervolume 0.692566\n",
      "9: training took 856.73 seconds w/ hypervolume 0.979538\n",
      "9: testing took 1.08 seconds w/ hypervolume 0.000000\n",
      "10: training took 1191.88 seconds w/ hypervolume 0.979410\n",
      "10: testing took 617.08 seconds w/ hypervolume 0.864981\n",
      "11: training took 73.04 seconds w/ hypervolume 0.979334\n",
      "11: testing took 0.41 seconds w/ hypervolume 0.000000\n",
      "12: training took 948.75 seconds w/ hypervolume 0.977650\n",
      "12: testing took 312.85 seconds w/ hypervolume 0.821001\n",
      "13: training took 101.29 seconds w/ hypervolume 0.000000\n",
      "13: testing took 1278.98 seconds w/ hypervolume 0.800371\n",
      "14: training took 224.12 seconds w/ hypervolume 0.978637\n",
      "14: testing took 0.69 seconds w/ hypervolume 0.000000\n",
      "15: training took 1184.53 seconds w/ hypervolume 0.978247\n",
      "15: testing took 1317.33 seconds w/ hypervolume 0.828927\n",
      "16: training took 83.50 seconds w/ hypervolume 0.980352\n",
      "16: testing took 0.54 seconds w/ hypervolume 0.000000\n",
      "17: training took 113.89 seconds w/ hypervolume 0.981271\n",
      "17: testing took 0.46 seconds w/ hypervolume 0.000000\n",
      "18: training took 131.21 seconds w/ hypervolume 0.978799\n",
      "18: testing took 0.50 seconds w/ hypervolume 0.000000\n",
      "19: training took 926.00 seconds w/ hypervolume 0.979975\n",
      "19: testing took 853.26 seconds w/ hypervolume 0.931201\n",
      "[[9.30078337e-01 2.13377179e-01            nan 4.65929094e+02]\n",
      " [4.79960634e-01 4.38303719e-01            nan 3.09161012e+02]]\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "# these lines define the problem, test problem, dataset, algorithm and number of sampling runs\n",
    "noOfVariables = 21\n",
    "noOfObjectives = 4\n",
    "\n",
    "problem = Problem(noOfVariables,noOfObjectives)\n",
    "problem.function = problem4_train\n",
    "\n",
    "testProblem = Problem(noOfVariables,noOfObjectives)\n",
    "testProblem.function = problem4_test\n",
    "\n",
    "dataset = 'bank'\n",
    "\n",
    "noOfSamplingRuns = 20\n",
    "\n",
    "X,y = getDataset(dataset)\n",
    "\n",
    "problem.types[:] = Real(-5,5)\n",
    "\n",
    "hypervolumeArray = np.zeros((noOfSamplingRuns, 2)) # col 0 for training results (hypervolume), col 1 for test results\n",
    "extremesArray = np.zeros((2, noOfObjectives, noOfObjectives, noOfSamplingRuns)) # the first dimension is for train/test\n",
    "averagePointArray = np.zeros((2, noOfSamplingRuns, noOfObjectives)) # the first dimension is for train/test. This array holds the average objective point for each run\n",
    "\n",
    "overallErrorRateSet = np.zeros((2, noOfSamplingRuns, len(targetMeasureArray)))\n",
    "\n",
    "# these will be used to store the algorithm that returns the largest hypervolume\n",
    "bestTrainAlgorithm = None\n",
    "bestTrainHypervolume = 0\n",
    "bestTestAlgorithm = None\n",
    "bestTestHypervolume = 0\n",
    "totalTrainTime = 0\n",
    "totalTestTime = 0\n",
    "\n",
    "for run in range(noOfSamplingRuns):\n",
    "\n",
    "    # generate new train/test split for each run\n",
    "    trainxs, testxs, trainys, testys  = train_test_split(X,y,train_size=0.8)  \n",
    "    \n",
    "    # training run\n",
    "    startTime = time.perf_counter()\n",
    "    \n",
    "    trainAlgorithm = CMAES(problem)\n",
    "#     trainAlgorithm = SPEA2(problem,population_size=500)\n",
    "    trainAlgorithm.run(10000)\n",
    "\n",
    "    trainHyp = Hypervolume(minimum=[0]*noOfObjectives,maximum=[1]*noOfObjectives)\n",
    "    trainHypResult = trainHyp(trainAlgorithm.result)  \n",
    "    hypervolumeArray[run, 0] = trainHypResult\n",
    "    \n",
    "    singleRunExtremesArray, averagePoint = findExtremes_averagePoint(trainAlgorithm)\n",
    "    extremesArray[0,:,:,run] = singleRunExtremesArray\n",
    "    averagePointArray[0,run,:] = averagePoint\n",
    "    \n",
    "    minErrorRateSet = minErrorRateSet_given_targetMeasuresArray(trainAlgorithm, targetMeasureArray)\n",
    "    overallErrorRateSet[0,run,:] = minErrorRateSet[:,0]\n",
    "    \n",
    "    endTime = time.perf_counter()\n",
    "    trainTime = endTime - startTime\n",
    "    totalTrainTime += trainTime\n",
    "    \n",
    "    print(\"{}: training took {:.2f} seconds w/ hypervolume {:2f}\".format(run, trainTime, trainHypResult))\n",
    "    \n",
    "    \n",
    "    # test run\n",
    "    testAlgorithm = deepcopy(trainAlgorithm)\n",
    "\n",
    "    startTime = time.perf_counter()\n",
    "\n",
    "    for resultNumber, result in enumerate(testAlgorithm.result):\n",
    "\n",
    "        w=result.variables\n",
    "        objectivesResult = testProblem.function(w)\n",
    "\n",
    "        # copy objective results individually to the testAlgorithm object\n",
    "        for i in range(len(objectivesResult)):  \n",
    "            testAlgorithm.result[resultNumber].objectives[i]=objectivesResult[i]\n",
    "\n",
    "    testHyp = Hypervolume(minimum=[0]*noOfObjectives,maximum=[1]*noOfObjectives)\n",
    "    testHypResult = testHyp(testAlgorithm.result)\n",
    "    hypervolumeArray[run, 1] = testHypResult\n",
    "    \n",
    "    singleRunExtremesArray, averagePoint = findExtremes_averagePoint(testAlgorithm)\n",
    "    extremesArray[1,:,:,run] = singleRunExtremesArray\n",
    "    averagePointArray[1,run,:] = averagePoint\n",
    "    \n",
    "    for targetMeasure in range(len(targetMeasureArray)):\n",
    "        if minErrorRateSet[targetMeasure,0] == 1.0:\n",
    "            overallErrorRateSet[1,run,targetMeasure] = 1.0\n",
    "        else:\n",
    "            overallErrorRateSet[1,run,targetMeasure] = testAlgorithm.result[np.int(minErrorRateSet[targetMeasure,1])].objectives[0]\n",
    "    \n",
    "    \n",
    "    endTime = time.perf_counter()\n",
    "    testTime = endTime - startTime\n",
    "    totalTestTime += testTime\n",
    "\n",
    "    print(\"{}: testing took {:.2f} seconds w/ hypervolume {:2f}\".format(run, testTime, testHypResult))\n",
    "    \n",
    "    if trainHypResult > bestTrainHypervolume:\n",
    "        bestTrainHypervolume = trainHypResult\n",
    "        bestTrainAlgorithm = deepcopy(trainAlgorithm)\n",
    "        \n",
    "        bestTestHypervolume = testHypResult\n",
    "        bestTestAlgorithm = deepcopy(testAlgorithm)\n",
    "        \n",
    "        bestData = [trainxs, testxs, trainys, testys]\n",
    "        bestDataSavePath = \"saved_data/\"+now.strftime(\"%Y%m%d_%H%M\")+\"_\"+str(problem.function)[10:18]+\"_\"+dataset+\"_\"+str(trainAlgorithm)[21:-26]+\"_data\"\n",
    "        np.save(bestDataSavePath, bestData)\n",
    "        \n",
    "# resultsSummary:\n",
    "# column 0: average hypervolume, column 1: hypervolume std, column 2: average point std (Euclidean distance), column 3: average time\n",
    "# row 0: training data, row 1: test data\n",
    "resultsSummary = np.zeros((2,4)) \n",
    "resultsMean = np.sum(hypervolumeArray, axis=0)/noOfSamplingRuns \n",
    "resultsSummary[:,0] = resultsMean\n",
    "resultsSummary[:,1] = np.sqrt(np.sum((hypervolumeArray - resultsMean)**2, axis=0)/noOfSamplingRuns)\n",
    "averagePointMeans = np.mean(averagePointArray,axis=1)\n",
    "resultsSummary[0,2] = np.std(np.sqrt(np.sum((averagePointArray[0,:,:]-averagePointMeans[0,:])**2,axis=1)))\n",
    "resultsSummary[1,2] = np.std(np.sqrt(np.sum((averagePointArray[1,:,:]-averagePointMeans[1,:])**2,axis=1)))\n",
    "resultsSummary[0,3] = totalTrainTime/noOfSamplingRuns\n",
    "resultsSummary[1,3] = totalTestTime/noOfSamplingRuns\n",
    "print(resultsSummary) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
